{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Matrix Decompositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many complex matrix operations cannot be solved efficiently or with stability using the limited\n",
    "precision of computers. Matrix decompositions are methods that reduce a matrix into constituent\n",
    "parts that make it easier to calculate more complex matrix operations. Matrix decomposition\n",
    "methods, also called matrix factorization methods, are a foundation of linear algebra in computers,\n",
    "even for basic operations such as solving systems of linear equations, calculating the inverse, and\n",
    "calculating the determinant of a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 What is a Matrix Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix decomposition is a way of reducing a matrix into its constituent parts. It is an approach that can simplify more complex matrix operations that can be performed on the decomposed matrix rather than on the original matrix itself. A common analogy for matrix decomposition is the factoring of numbers, such as the factoring of 10 into 2 x 5. \n",
    "\n",
    "For this reason, matrix decomposition is also called matrix factorization. Like factoring real values, there are many ways to decompose a matrix, hence there are a range of different matrix decomposition techniques. Two simple and widely used matrix decomposition methods are the LU matrix decomposition and the QR matrix decomposition. Next, we will take a closer look at each of these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 LU Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LU decomposition is for square matrices and decomposes a matrix into L and U components.\n",
    "\n",
    "$$\n",
    "A = L \\cdot U\n",
    "$$\n",
    "\n",
    "Or, without the dot notation.\n",
    "\n",
    "$$\n",
    "A = LU \n",
    "$$\n",
    "\n",
    "Where $A$ is the square matrix that we wish to decompose, $L$ is the lower triangle matrix and $U$ is the upper triangle matrix.\n",
    "\n",
    "The factors $L$ and $U$ are triangular matrices. The factorization that comes from elimination is $A = LU$.\n",
    "\n",
    "— Page 97, Introduction to Linear Algebra, Fifth Edition, 2016.\n",
    "\n",
    "The LU decomposition is found using an iterative numerical process and can fail for those matrices that cannot be decomposed or decomposed easily. A variation of this decomposition that is numerically more stable to solve in practice is called the LUP decomposition, or the LU decomposition with partial pivoting.\n",
    "\n",
    "$$\n",
    "A = L \\cdot U \\cdot P\n",
    "$$\n",
    "\n",
    "The rows of the parent matrix are re-ordered to simplify the decomposition process and the additional $P$ matrix specifies a way to permute the result or return the result to the original order. There are also other variations of the LU. The LU decomposition is often used to simplify the solving of systems of linear equations, such as finding the coefficients in a linear regression, as well as in calculating the determinant and inverse of a matrix.\n",
    "\n",
    "The LU decomposition can be implemented in Python with the `lu()` function. More specifically, this function calculates an LPU decomposition. The example below first defines a $3 \\times 3$ square matrix. The LU decomposition is calculated, then the original matrix is reconstructed from the components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n",
      "\n",
      "[[1.         0.         0.        ]\n",
      " [0.14285714 1.         0.        ]\n",
      " [0.57142857 0.5        1.        ]]\n",
      "\n",
      "[[7.         8.         9.        ]\n",
      " [0.         0.85714286 1.71428571]\n",
      " [0.         0.         0.        ]]\n",
      "\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "# LU decomposition\n",
    "from numpy import array\n",
    "from scipy.linalg import lu\n",
    "# define a square matrix\n",
    "A = array([\n",
    "[1, 2, 3],\n",
    "[4, 5, 6],\n",
    "[7, 8, 9]])\n",
    "print(A)\n",
    "# factorize\n",
    "P, L, U = lu(A)\n",
    "print(f'\\r\\n{P}')\n",
    "print(f'\\r\\n{L}')\n",
    "print(f'\\r\\n{U}')\n",
    "# reconstruct\n",
    "B = P.dot(L).dot(U)\n",
    "print(f'\\r\\n{B}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 QR Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The QR decomposition is for $n \\times m$ matrices (not limited to square matrices) and decomposes a matrix into $Q$ and $R$ components.\n",
    "\n",
    "$$\n",
    "A = Q \\cdot R\n",
    "$$\n",
    "\n",
    "Or, without the dot notation.\n",
    "\n",
    "$$\n",
    "A = QR \n",
    "$$\n",
    "\n",
    "Where $A$ is the matrix that we wish to decompose, $Q$ a matrix with the size $m \\times m$, and $R$ is an upper triangle matrix with the size $m \\times n$. The QR decomposition is found using an iterative numerical method that can fail for those matrices that cannot be decomposed, or decomposed easily. Like the LU decomposition, the QR decomposition is often used to solve systems of linear equations, although it is not limited to square matrices.\n",
    "\n",
    "The QR decomposition can be implemented in NumPy using the `qr()` function. By default, the function returns the $Q$ and $R$ matrices with smaller or reduced dimensions that is more economical.\n",
    "\n",
    "We can change this to return the expected sizes of $m \\times m$ for $Q$ and $m \\times n$ for $R$ by specifying the mode argument as `complete`, although this is not required for most applications. The example below defines a $3 \\times 2$ matrix, calculates the QR decomposition, then reconstructs the original matrix from the decomposed elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "\n",
      "[[-0.16903085  0.89708523  0.40824829]\n",
      " [-0.50709255  0.27602622 -0.81649658]\n",
      " [-0.84515425 -0.34503278  0.40824829]]\n",
      "\n",
      "[[-5.91607978 -7.43735744]\n",
      " [ 0.          0.82807867]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# QR decomposition\n",
    "from numpy import array\n",
    "from numpy.linalg import qr\n",
    "# define rectangular matrix\n",
    "A = array([\n",
    "[1, 2],\n",
    "[3, 4],\n",
    "[5, 6]])\n",
    "print(A)\n",
    "# factorize\n",
    "Q, R = qr(A, 'complete')\n",
    "print(f'\\r\\n{Q}')\n",
    "print(f'\\r\\n{R}')\n",
    "# reconstruct\n",
    "B = Q.dot(R)\n",
    "print(f'\\r\\n{B}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4 Cholesky Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cholesky decomposition is for square symmetric matrices where all values are greater than zero, so-called positive definite matrices. For our interests in machine learning, we will focus on the Cholesky decomposition for real-valued matrices and ignore the cases when working with complex numbers. The decomposition is defined as follows:\n",
    "\n",
    "$$\n",
    "A = L \\cdot L^T\n",
    "$$\n",
    "\n",
    "Or without the dot notation:\n",
    "\n",
    "$$\n",
    "A = LL^T\n",
    "$$\n",
    "\n",
    "Where $A$ is the matrix being decomposed, $L$ is the lower triangular matrix and $L^T$ is the transpose of $L$. The decompose can also be written as the product of the upper triangular matrix, for example:\n",
    "\n",
    "$$\n",
    "A = U^T \\cdot U\n",
    "$$\n",
    "\n",
    "Where $U$ is the upper triangular matrix. The Cholesky decomposition is used for solving linear least squares for linear regression, as well as simulation and optimization methods. When decomposing symmetric matrices, the Cholesky decomposition is nearly twice as efficient as the LU decomposition and should be preferred in these cases.\n",
    "\n",
    "While symmetric, positive definite matrices are rather special, they occur quite frequently in some applications, so their special factorization, called Cholesky decomposition, is good to know about. When you can use it, Cholesky decomposition is about a factor of two faster than alternative methods for solving linear equations.\n",
    "\n",
    "— Page 100, Numerical Recipes: The Art of Scientific Computing, Third Edition, 2007.\n",
    "\n",
    "The Cholesky decomposition can be implemented in NumPy by calling the `cholesky()` function. The function only returns $L$ as we can easily access the $L$ transpose as needed. The example below defines a $3 \\times 3$ symmetric and positive definite matrix and calculates the Cholesky decomposition, then the original matrix is reconstructed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first prints the symmetric matrix, then the lower triangular matrix\n",
    "from the decomposition followed by the reconstructed matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 1]\n",
      " [1 2 1]\n",
      " [1 1 2]]\n",
      "\n",
      "[[1.41421356 0.         0.        ]\n",
      " [0.70710678 1.22474487 0.        ]\n",
      " [0.70710678 0.40824829 1.15470054]]\n",
      "\n",
      "[[2. 1. 1.]\n",
      " [1. 2. 1.]\n",
      " [1. 1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "# Cholesky decomposition\n",
    "from numpy import array\n",
    "from numpy.linalg import cholesky\n",
    "# define symmetrical matrix\n",
    "A = array([\n",
    "[2, 1, 1],\n",
    "[1, 2, 1],\n",
    "[1, 1, 2]])\n",
    "print(A)\n",
    "# factorize\n",
    "L = cholesky(A)\n",
    "print(f'\\r\\n{L}')\n",
    "# reconstruct\n",
    "B = L.dot(L.T)\n",
    "print(f'\\r\\n{B}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix decompositions are a useful tool for reducing a matrix to their constituent parts in\n",
    "order to simplify a range of more complex operations. Perhaps the most used type of matrix\n",
    "decomposition is the eigendecomposition that decomposes a matrix into eigenvectors and\n",
    "eigenvalues. This decomposition also plays a role in methods used in machine learning, such\n",
    "as in the Principal Component Analysis method or PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1 Eigendecomposition of a Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigendecomposition of a matrix is a type of decomposition that involves decomposing a square matrix into a set of eigenvectors and eigenvalues.\n",
    "\n",
    "One of the most widely used kinds of matrix decomposition is called eigendecomposition, in which we decompose a matrix into a set of eigenvectors and eigenvalues.\n",
    "\n",
    "— Page 42, Deep Learning, 2016.\n",
    "\n",
    "A vector is an eigenvector of a matrix if it satisfies the following equation.\n",
    "\n",
    "$$\n",
    "A \\cdot v = \\lambda \\cdot v \n",
    "$$\n",
    "\n",
    "This is called the eigenvalue equation, where $A$ is the parent square matrix that we are decomposing, $v$ is the eigenvector of the matrix, and $\\lambda$ is the lowercase Greek letter lambda and represents the eigenvalue scalar. Or without the dot notation.\n",
    "\n",
    "$$\n",
    "Av = \\lambda v \n",
    "$$\n",
    "\n",
    "A matrix could have one eigenvector and eigenvalue for each dimension of the parent matrix. Not all square matrices can be decomposed into eigenvectors and eigenvalues, and some can only be decomposed in a way that requires complex numbers. The parent matrix can be shown to be a product of the eigenvectors and eigenvalues.\n",
    "\n",
    "$$\n",
    "A = Q \\cdot \\Lambda \\cdot Q^T\n",
    "$$\n",
    "\n",
    "Or, without the dot notation.\n",
    "\n",
    "$$\n",
    "A = Q \\Lambda Q^T\n",
    "$$\n",
    "\n",
    "Where $Q$ is a matrix comprised of the eigenvectors, $\\Lambda$ is the uppercase Greek letter lambda and is the diagonal matrix comprised of the eigenvalues, and $Q^T$ is the transpose of the matrix comprised of the eigenvectors.\n",
    "\n",
    "However, if we want to decompose matrices into their eigenvalues and eigenvectors. Doing so can help us to analyze certain properties of the matrix, much as decomposing an integer into its prime factors can help us understand the behavior of the integer.\n",
    "\n",
    "— Page 43, Deep Learning, 2016.\n",
    "\n",
    "Eigen is not a name, e.g. the method is not named after “Eigen”; eigen (pronounced eye-gan) is a German word that means own or innate, as in belonging to the parent matrix. A decomposition operation does not result in a compression of the matrix; instead, it breaks it down into constituent parts to make certain operations on the matrix easier to perform. Like other matrix decomposition methods, Eigendecomposition is used as an element to simplify the calculation of other more complex matrix operations.\n",
    "\n",
    "15.3. Eigenvectors and Eigenvalues\n",
    "\n",
    "Almost all vectors change direction, when they are multiplied by $A$. Certain exceptional vectors $x$ are in the same direction as $Ax$. Those are the “eigenvectors”. Multiply an eigenvector by $A$, and the vector $Ax$ is the number $\\lambda$ times the original $x$. [...] The eigenvalue $\\lambda$ tells whether the special vector $x$ is stretched or shrunk or reversed or left unchanged — when it is multiplied by $A$.\n",
    "\n",
    "— Page 289, Introduction to Linear Algebra, Fifth Edition, 2016.\n",
    "\n",
    "Eigendecomposition can also be used to calculate the principal components of a matrix in the Principal Component Analysis method or PCA that can be used to reduce the dimensionality of data in machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2 Eigenvectors and Eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigenvectors are unit vectors, which means that their length or magnitude is equal to 1.0. They are often referred as right vectors, which simply means a column vector (as opposed to a row vector or a left vector). A right-vector is a vector as we understand them. Eigenvalues are coefficients applied to eigenvectors that give the vectors their length or magnitude. For example, a negative eigenvalue may reverse the direction of the eigenvector as part of scaling it. A matrix that has only positive eigenvalues is referred to as a positive definite matrix, whereas if the eigenvalues are all negative, it is referred to as a negative definite matrix.\n",
    "\n",
    "Decomposing a matrix in terms of its eigenvalues and its eigenvectors gives valuable insights into the properties of the matrix. Certain matrix calculations, like computing the power of the matrix, become much easier when we use the eigendecomposition of the matrix.\n",
    "\n",
    "— Page 262, No Bullshit Guide to Linear Algebra, 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.3 Calculation of Eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An eigendecomposition is calculated on a square matrix using an efficient iterative algorithm, of which we will not go into the details. Often an eigenvalue is found first, then an eigenvector is found to solve the equation as a set of coefficients. The eigendecomposition can be calculated in NumPy using the `eig()` function. The example below first defines a $3 \\times 3$ square matrix. The eigendecomposition is calculated on the matrix returning the eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first prints the defined matrix, followed by the eigenvalues and the\n",
    "eigenvectors. More specifically, the eigenvectors are the right-hand side eigenvectors and are\n",
    "normalized to unit length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "[ 1.61168440e+01 -1.11684397e+00 -1.30367773e-15]\n",
      "\n",
      "[[-0.23197069 -0.78583024  0.40824829]\n",
      " [-0.52532209 -0.08675134 -0.81649658]\n",
      " [-0.8186735   0.61232756  0.40824829]]\n"
     ]
    }
   ],
   "source": [
    "# eigendecomposition\n",
    "from numpy import array\n",
    "from numpy.linalg import eig\n",
    "# define matrix\n",
    "A = array([\n",
    "[1, 2, 3],\n",
    "[4, 5, 6],\n",
    "[7, 8, 9]])\n",
    "print(A)\n",
    "# factorize\n",
    "values, vectors = eig(A)\n",
    "print(f'\\r\\n{values}')\n",
    "print(f'\\r\\n{vectors}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4 Confirm an Eigenvector and Eigenvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that a vector is indeed an eigenvector of a matrix. We do this by multiplying\n",
    "the candidate eigenvector by the value vector and comparing the result with the eigenvalue.\n",
    "First, we will define a matrix, then calculate the eigenvalues and eigenvectors. \n",
    "\n",
    "We will then test whether the first vector and value are in fact an eigenvalue and eigenvector for the matrix. We\n",
    "know they are, but it is a good exercise.\n",
    "\n",
    "The eigenvectors are returned as a matrix with the same dimensions as the parent matrix,\n",
    "where each column is an eigenvector, e.g. the first eigenvector is vectors[:, 0]. Eigenvalues\n",
    "are returned as a list, where value indices in the returned array are paired with eigenvectors\n",
    "by column index, e.g. the first eigenvalue at values[0] is paired with the first eigenvector at\n",
    "vectors[:, 0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -3.73863537  -8.46653421 -13.19443305]\n",
      "\n",
      "[ -3.73863537  -8.46653421 -13.19443305]\n"
     ]
    }
   ],
   "source": [
    "# confirm eigenvector\n",
    "from numpy import array\n",
    "from numpy.linalg import eig\n",
    "# define matrix\n",
    "A = array([\n",
    "[1, 2, 3],\n",
    "[4, 5, 6],\n",
    "[7, 8, 9]])\n",
    "# factorize\n",
    "values, vectors = eig(A)\n",
    "# confirm first eigenvector\n",
    "B = A.dot(vectors[:, 0])\n",
    "print(B)\n",
    "C = vectors[:, 0] * values[0]\n",
    "print(f'\\r\\n{C}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example multiplies the original matrix with the first eigenvector and compares it to the\n",
    "first eigenvector multiplied by the first eigenvalue. Running the example prints the results of\n",
    "these two multiplications that show the same resulting vector, as we would expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.5 Reconstruct Matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reverse the process and reconstruct the original matrix given only the eigenvectors and\n",
    "eigenvalues. First, the list of eigenvectors must be taken together as a matrix, where each vector\n",
    "becomes a row. The eigenvalues need to be arranged into a diagonal matrix. The NumPy\n",
    "diag() function can be used for this. Next, we need to calculate the inverse of the eigenvector\n",
    "matrix, which we can achieve with the inv() NumPy function. Finally, these elements need to\n",
    "be multiplied together with the dot() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "# reconstruct matrix\n",
    "from numpy import diag\n",
    "from numpy.linalg import inv\n",
    "from numpy import array\n",
    "from numpy.linalg import eig\n",
    "# define matrix\n",
    "A = array([\n",
    "[1, 2, 3],\n",
    "[4, 5, 6],\n",
    "[7, 8, 9]])\n",
    "print(A)\n",
    "# factorize\n",
    "values, vectors = eig(A)\n",
    "# create matrix from eigenvectors\n",
    "Q = vectors\n",
    "# create inverse of eigenvectors matrix\n",
    "R = inv(Q)\n",
    "# create diagonal matrix from eigenvalues\n",
    "L = diag(values)\n",
    "# reconstruct the original matrix\n",
    "B = Q.dot(L).dot(R)\n",
    "print(f'\\r\\n{B}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example calculates the eigenvalues and eigenvectors again and uses them to reconstruct\n",
    "the original matrix. Running the example first prints the original matrix, then the matrix\n",
    "reconstructed from eigenvalues and eigenvectors matching the original matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix decomposition, also known as matrix factorization, involves describing a given matrix\n",
    "using its constituent elements. Perhaps the most known and widely used matrix decomposition\n",
    "method is the Singular-Value Decomposition, or SVD. All matrices have an SVD, which makes\n",
    "it more stable than other methods, such as the eigendecomposition. As such, it is often used\n",
    "in a wide array of applications including compressing, denoising, and data reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.1 What is the Singular-Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Singular-Value Decomposition, or SVD for short, is a matrix decomposition method for reducing a matrix to its constituent parts in order to make certain subsequent matrix calculations simpler. For the case of simplicity we will focus on the SVD for real-valued matrices and ignore the case for complex numbers.\n",
    "\n",
    "$$\n",
    "A = U \\cdot \\Sigma \\cdot V^T\n",
    "$$\n",
    "\n",
    "Where $A$ is the real $n \\times m$ matrix that we wish to decompose, $U$ is an $m \\times m$ matrix, $\\Sigma$ represented by the uppercase Greek letter sigma) is an $m \\times n$ diagonal matrix, and $V^T$ is the $V$ transpose of an $n \\times n$ matrix where $T$ is a superscript.\n",
    "\n",
    "The Singular Value Decomposition is a highlight of linear algebra.\n",
    "\n",
    "— Page 371, Introduction to Linear Algebra, 2016.\n",
    "\n",
    "The diagonal values in the $\\Sigma$ matrix are known as the singular values of the original matrix $A$. The columns of the $U$ matrix are called the left-singular vectors of $A$, and the columns of $V$ are called the right-singular vectors of $A$. The SVD is calculated via iterative numerical methods. We will not go into the details of these methods. Every rectangular matrix has a singular value decomposition, although the resulting matrices may contain complex numbers and the limitations of floating point arithmetic may cause some matrices to fail to decompose neatly.\n",
    "\n",
    "The singular value decomposition (SVD) provides another way to factorize a matrix, into singular vectors and singular values. The SVD allows us to discover some of the same kind of information as the eigendecomposition. However, the SVD is more generally applicable.\n",
    "\n",
    "— Pages 44-45, Deep Learning, 2016.\n",
    "\n",
    "The SVD is used widely both in the calculation of other matrix operations, such as matrix inverse, but also as a data reduction method in machine learning. SVD can also be used in least squares linear regression, image compression, and denoising data.\n",
    "\n",
    "The singular value decomposition (SVD) has numerous applications in statistics, machine learning, and computer science. Applying the SVD to a matrix is like looking inside it with X-ray vision...\n",
    "\n",
    "— Page 297, No Bullshit Guide to Linear Algebra, 2017.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.2 Calculate Singular-Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD can be calculated by calling the `svd()` function. The function takes a matrix and returns the $U$, $\\Sigma$ and $V^T$ elements. The $\\Sigma$ diagonal matrix is returned as a vector of singular values. The $V$ matrix is returned in a transposed form, e.g. $V^T$. The example below defines a $3 \\times 2$ matrix and calculates the singular-value decomposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "\n",
      "[[-0.2298477   0.88346102  0.40824829]\n",
      " [-0.52474482  0.24078249 -0.81649658]\n",
      " [-0.81964194 -0.40189603  0.40824829]]\n",
      "\n",
      "[9.52551809 0.51430058]\n",
      "\n",
      "[[-0.61962948 -0.78489445]\n",
      " [-0.78489445  0.61962948]]\n"
     ]
    }
   ],
   "source": [
    "# singular-value decomposition\n",
    "from numpy import array\n",
    "from scipy.linalg import svd\n",
    "# define a matrix\n",
    "A = array([\n",
    "[1, 2],\n",
    "[3, 4],\n",
    "[5, 6]])\n",
    "print(A)\n",
    "# factorize\n",
    "U, s, V = svd(A)\n",
    "print(f'\\r\\n{U}')\n",
    "print(f'\\r\\n{s}')\n",
    "print(f'\\r\\n{V}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first prints the defined $3 \\times 2$ matrix, then the $3 \\times 3$ $U$ matrix, 2 element $\\Sigma$ vector, and $2 \\times 2$ $V^T$ matrix elements calculated from the decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.3 Reconstruct Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original matrix can be reconstructed from the $U$, $\\Sigma$, and $V^T$ elements. The $U$, $s$, and $V$ elements returned from the `svd()` cannot be multiplied directly. The $s$ vector must be converted into a diagonal matrix using the `diag()` function. By default, this function will create a square matrix that is $m \\times m$, relative to our original matrix. This causes a problem as the size of the matrices do not fit the rules of matrix multiplication, where the number of columns in a matrix must match the number of rows in the subsequent matrix. After creating the square $\\Sigma$ diagonal matrix, the sizes of the matrices are relative to the original $n \\times m$ matrix that we are decomposing, as follows:\n",
    "\n",
    "$$\n",
    "U(m \\times m) \\cdot \\Sigma (m \\times m) \\cdot V^T (n \\times n)\n",
    "$$\n",
    "\n",
    "Where, in fact, we require:\n",
    "\n",
    "$$\n",
    "U(m \\times m) \\cdot \\Sigma (m \\times n) \\cdot V^T (n \\times n)\n",
    "$$\n",
    "\n",
    "We can achieve this by creating a new $\\Sigma$ matrix of all zero values that is $m \\times n$ (e.g. more rows) and populate the first $n \\times n$ part of the matrix with the square diagonal matrix calculated via `diag()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]\n",
      " [5 6]]\n",
      "\n",
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# reconstruct rectangular matrix from svd\n",
    "from numpy import array\n",
    "from numpy import diag\n",
    "from numpy import zeros\n",
    "from scipy.linalg import svd\n",
    "# define matrix\n",
    "A = array([\n",
    "[1, 2],\n",
    "[3, 4],\n",
    "[5, 6]])\n",
    "print(A)\n",
    "# factorize\n",
    "U, s, V = svd(A)\n",
    "# create m x n Sigma matrix\n",
    "Sigma = zeros((A.shape[0], A.shape[1]))\n",
    "# populate Sigma with n x n diagonal matrix\n",
    "Sigma[:A.shape[1], :A.shape[1]] = diag(s)\n",
    "# reconstruct matrix\n",
    "B = U.dot(Sigma.dot(V))\n",
    "print(f'\\r\\n{B}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first prints the original matrix, then the matrix reconstructed from\n",
    "the SVD elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above complication with the Σ diagonal only exists with the case where m and n are\n",
    "not equal. The diagonal matrix can be used directly when reconstructing a square matrix, as\n",
    "follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]\n",
      " [7. 8. 9.]]\n"
     ]
    }
   ],
   "source": [
    "# reconstruct square matrix from svd\n",
    "from numpy import array\n",
    "from numpy import diag\n",
    "from scipy.linalg import svd\n",
    "# define matrix\n",
    "A = array([\n",
    "[1, 2, 3],\n",
    "[4, 5, 6],\n",
    "[7, 8, 9]])\n",
    "print(A)\n",
    "# factorize\n",
    "U, s, V = svd(A)\n",
    "# create n x n Sigma matrix\n",
    "Sigma = diag(s)\n",
    "# reconstruct matrix\n",
    "B = U.dot(Sigma.dot(V))\n",
    "print(f'\\r\\n{B}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.4 Pseudoinverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pseudoinverse is the generalization of the matrix inverse for square matrices to rectangular matrices where the number of rows and columns are not equal. It is also called the Moore-Penrose Inverse after two independent discoverers of the method or the Generalized Inverse.\n",
    "\n",
    "Matrix inversion is not defined for matrices that are not square. [...] When $A$ has more columns than rows, then solving a linear equation using the pseudoinverse provides one of the many possible solutions.\n",
    "\n",
    "— Page 46, Deep Learning, 2016.\n",
    "\n",
    "The pseudoinverse is denoted as $A^+$, where $A$ is the matrix that is being inverted and $+$ is a superscript. The pseudoinverse is calculated using the singular value decomposition of $A$:\n",
    "\n",
    "$$\n",
    "A^+ = V \\cdot D^+ \\cdot U^T\n",
    "$$\n",
    "\n",
    "Or, without the dot notation:\n",
    "\n",
    "$$\n",
    "A^+ = V D^+ U^T\n",
    "$$\n",
    "\n",
    "Where $A^+$ is the pseudoinverse, $D^+$ is the pseudoinverse of the diagonal matrix $\\Sigma$ and $V^T$ is the transpose of $V$. We can get $U$ and $V$ from the SVD operation.\n",
    "\n",
    "$$\n",
    "A = U \\cdot \\Sigma \\cdot V^T\n",
    "$$\n",
    "\n",
    "The $D^+$ can be calculated by creating a diagonal matrix from $\\Sigma$, calculating the reciprocal of each non-zero element in $\\Sigma$, and taking the transpose if the original matrix was rectangular.\n",
    "\n",
    "$$\n",
    "\\Sigma = \\begin{pmatrix}\n",
    "s_{1,1} & 0 & 0 \\\\\n",
    "0 & s_{2,2} & 0 \\\\\n",
    "0 & 0 & s_{3,3}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "16.5. Pseudoinverse\n",
    "\n",
    "$$\n",
    "D^+ = \\begin{pmatrix}\n",
    "\\frac{1}{s_{1,1}} & 0 & 0 \\\\\n",
    "0 & \\frac{1}{s_{2,2}} & 0 \\\\\n",
    "0 & 0 & \\frac{1}{s_{3,3}}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The pseudoinverse provides one way of solving the linear regression equation, specifically when there are more rows than there are columns, which is often the case. NumPy provides the function `pinv()` for calculating the pseudoinverse of a rectangular matrix. The example below defines a $4 \\times 2$ matrix and calculates the pseudoinverse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1 0.2]\n",
      " [0.3 0.4]\n",
      " [0.5 0.6]\n",
      " [0.7 0.8]]\n",
      "\n",
      "[[-1.00000000e+01 -5.00000000e+00  1.42385628e-14  5.00000000e+00]\n",
      " [ 8.50000000e+00  4.50000000e+00  5.00000000e-01 -3.50000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# pseudoinverse\n",
    "from numpy import array\n",
    "from numpy.linalg import pinv\n",
    "# define matrix\n",
    "A = array([\n",
    "[0.1, 0.2],\n",
    "[0.3, 0.4],\n",
    "[0.5, 0.6],\n",
    "[0.7, 0.8]])\n",
    "print(A)\n",
    "# calculate pseudoinverse\n",
    "B = pinv(A)\n",
    "print(f'\\r\\n{B}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the pseudoinverse manually via the SVD and compare the results to the\n",
    "pinv() function. First we must calculate the SVD. Next we must calculate the reciprocal of\n",
    "each value in the s array. Then the s array can be transformed into a diagonal matrix with an\n",
    "added row of zeros to make it rectangular. Finally, we can calculate the pseudoinverse from the\n",
    "elements. The specific implementation is:\n",
    "\n",
    "$$\n",
    "A^+ = V^T \\cdot D^T \\cdot U^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1 0.2]\n",
      " [0.3 0.4]\n",
      " [0.5 0.6]\n",
      " [0.7 0.8]]\n",
      "\n",
      "[[-1.00000000e+01 -5.00000000e+00  1.42578328e-14  5.00000000e+00]\n",
      " [ 8.50000000e+00  4.50000000e+00  5.00000000e-01 -3.50000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# pseudoinverse via svd\n",
    "from numpy import array\n",
    "from numpy.linalg import svd\n",
    "from numpy import zeros\n",
    "from numpy import diag\n",
    "# define matrix\n",
    "A = array([\n",
    "[0.1, 0.2],\n",
    "[0.3, 0.4],\n",
    "[0.5, 0.6],\n",
    "[0.7, 0.8]])\n",
    "print(A)\n",
    "# factorize\n",
    "U, s, V = svd(A)\n",
    "# reciprocals of s\n",
    "d = 1.0 / s\n",
    "# create m x n D matrix\n",
    "D = zeros(A.shape)\n",
    "# populate D with n x n diagonal matrix\n",
    "D[:A.shape[1], :A.shape[1]] = diag(d)\n",
    "# calculate pseudoinverse\n",
    "B = V.T.dot(D.T).dot(U.T)\n",
    "print(f'\\r\\n{B}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13.5 Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular application of SVD is for dimensionality reduction. Data with a large number of features, such as more features (columns) than observations (rows) may be reduced to a smaller subset of features that are most relevant to the prediction problem. The result is a matrix with a lower rank that is said to approximate the original matrix. To do this we can perform an SVD operation on the original data and select the top $k$ largest singular values in $\\Sigma$. These columns can be selected from $\\Sigma$ and the rows selected from $V^T$. An approximate $B$ of the original vector $A$ can then be reconstructed.\n",
    "\n",
    "$$\n",
    "B = U \\cdot \\Sigma_k \\cdot V_k^T \n",
    "$$\n",
    "\n",
    "In natural language processing, this approach can be used on matrices of word occurrences or word frequencies in documents and is called Latent Semantic Analysis or Latent Semantic Indexing. In practice, we can retain and work with a descriptive subset of the data called $T$. This is a dense summary of the matrix or a projection.\n",
    "\n",
    "$$\n",
    "T = U \\cdot \\Sigma_k \n",
    "$$\n",
    "\n",
    "16.6. Dimensionality Reduction\n",
    "\n",
    "Further, this transform can be calculated and applied to the original matrix $A$ as well as other similar matrices.\n",
    "\n",
    "$$\n",
    "T = A \\cdot V_k^T \n",
    "$$\n",
    "\n",
    "The example below demonstrates data reduction with the SVD. First a $3 \\times 10$ matrix is defined, with more columns than rows. The SVD is calculated and only the first two features are selected. The elements are recombined to give an accurate reproduction of the original matrix. Finally the transform is calculated two different ways.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first prints the defined matrix then the reconstructed approximation,\n",
    "followed by two equivalent transforms of the original matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5  6  7  8  9 10]\n",
      " [11 12 13 14 15 16 17 18 19 20]\n",
      " [21 22 23 24 25 26 27 28 29 30]]\n",
      "\n",
      "[[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n",
      " [11. 12. 13. 14. 15. 16. 17. 18. 19. 20.]\n",
      " [21. 22. 23. 24. 25. 26. 27. 28. 29. 30.]]\n",
      "\n",
      "[[-18.52157747   6.47697214]\n",
      " [-49.81310011   1.91182038]\n",
      " [-81.10462276  -2.65333138]]\n",
      "\n",
      "[[-18.52157747   6.47697214]\n",
      " [-49.81310011   1.91182038]\n",
      " [-81.10462276  -2.65333138]]\n"
     ]
    }
   ],
   "source": [
    "# data reduction with svd\n",
    "from numpy import array\n",
    "from numpy import diag\n",
    "from numpy import zeros\n",
    "from scipy.linalg import svd\n",
    "# define matrix\n",
    "A = array([\n",
    "[1,2,3,4,5,6,7,8,9,10],\n",
    "[11,12,13,14,15,16,17,18,19,20],\n",
    "[21,22,23,24,25,26,27,28,29,30]])\n",
    "print(A)\n",
    "# factorize\n",
    "U, s, V = svd(A)\n",
    "# create m x n Sigma matrix\n",
    "Sigma = zeros((A.shape[0], A.shape[1]))\n",
    "# populate Sigma with n x n diagonal matrix\n",
    "Sigma[:A.shape[0], :A.shape[0]] = diag(s)\n",
    "# select\n",
    "n_elements = 2\n",
    "Sigma = Sigma[:, :n_elements]\n",
    "V = V[:n_elements, :]\n",
    "# reconstruct\n",
    "B = U.dot(Sigma.dot(V))\n",
    "print(f'\\r\\n{B}')\n",
    "# transform\n",
    "T = U.dot(Sigma)\n",
    "print(f'\\r\\n{T}')\n",
    "T = A.dot(V.T)\n",
    "print(f'\\r\\n{T}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn provides a `TruncatedSVD` class that implements this capability directly. The `TruncatedSVD` class can be created in which you must specify the number of desirable features or components to select, e.g. 2. Once created, you can fit the transform (e.g. calculate $V_k^T$) by calling the `fit()` function, then apply it to the original matrix by calling the `transform()` function. The result is the transform of $A$ called $T$ above. The example below demonstrates the `TruncatedSVD` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example first prints the defined matrix, followed by the transformed version\n",
    "of the matrix. We can see that the values match those calculated manually above, except for\n",
    "the sign on some values. We can expect there to be some instability when it comes to the\n",
    "sign given the nature of the calculations involved and the differences in the underlying libraries\n",
    "and methods used. This instability of sign should not be a problem in practice as long as the\n",
    "transform is trained for reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5  6  7  8  9 10]\n",
      " [11 12 13 14 15 16 17 18 19 20]\n",
      " [21 22 23 24 25 26 27 28 29 30]]\n",
      "\n",
      "[[18.52157747  6.47697214]\n",
      " [49.81310011  1.91182038]\n",
      " [81.10462276 -2.65333138]]\n"
     ]
    }
   ],
   "source": [
    "# svd data reduction in scikit-learn\n",
    "from numpy import array\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# define matrix\n",
    "A = array([\n",
    "[1,2,3,4,5,6,7,8,9,10],\n",
    "[11,12,13,14,15,16,17,18,19,20],\n",
    "[21,22,23,24,25,26,27,28,29,30]])\n",
    "print(A)\n",
    "# create transform\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "# fit transform\n",
    "svd.fit(A)\n",
    "# apply transform\n",
    "result = svd.transform(A)\n",
    "print(f'\\r\\n{result}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
